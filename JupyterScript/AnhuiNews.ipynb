{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.7 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "1baa965d5efe3ac65b79dfc60c0d706280b1da80fedb7760faf2759126c4f253"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "begin to save......\nsave success!\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import xlwt\n",
    "\n",
    "savePath =\"安徽新闻网链接.xls\"\n",
    "def main():\n",
    "    url = \"http://www.ahnews.com.cn/\"\n",
    "    collectUrl(url)\n",
    "def collectUrl(url):\n",
    "    header = {\n",
    "        \"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.141 Safari/537.36 Edg/87.0.664.75\"\n",
    "    }\n",
    "    req = urllib.request.Request(url=url,headers=header)\n",
    "    res = urllib.request.urlopen(req)\n",
    "    html = res.read().decode(\"utf-8\")\n",
    "    pageContent = BeautifulSoup(html,\"html.parser\")\n",
    "    pageContent = str(pageContent)\n",
    "    news =re.compile(r'<li><a href=\"http://(.*?).html\" target=\"_blank\">')\n",
    "    data = re.findall(news,pageContent)\n",
    "    data = [\"http://\"+i+'.html' for i in data]\n",
    "    print(\"begin to save......\")\n",
    "    book = xlwt.Workbook(encoding = \"utf-8\",style_compression=0)\n",
    "    sheet = book.add_sheet(\"安徽新闻网新闻链接\",cell_overwrite_ok=True)\n",
    "    col = (\"新闻链接\")\n",
    "    sheet.write(0,0,col)\n",
    "    for i in range(len(data)):\n",
    "        sheet.write(i+1,0,data[i])\n",
    "    book.save(savePath)\n",
    "    print(\"save success!\")\n",
    "main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "共69篇新闻稿\n",
      "正在下载1篇\n",
      "正在下载2篇\n",
      "正在下载3篇\n",
      "正在下载4篇\n",
      "正在下载5篇\n",
      "正在下载6篇\n",
      "正在下载7篇\n",
      "正在下载8篇\n",
      "正在下载9篇\n",
      "正在下载10篇\n",
      "正在下载11篇\n",
      "正在下载12篇\n",
      "正在下载13篇\n",
      "正在下载14篇\n",
      "正在下载15篇\n",
      "正在下载16篇\n",
      "正在下载17篇\n",
      "正在下载18篇\n",
      "正在下载19篇\n",
      "正在下载20篇\n",
      "正在下载21篇\n",
      "正在下载22篇\n",
      "正在下载23篇\n",
      "正在下载24篇\n",
      "正在下载25篇\n",
      "正在下载26篇\n",
      "正在下载27篇\n",
      "正在下载28篇\n",
      "正在下载29篇\n",
      "正在下载30篇\n",
      "正在下载31篇\n",
      "正在下载32篇\n",
      "正在下载33篇\n",
      "正在下载34篇\n",
      "正在下载35篇\n",
      "正在下载36篇\n",
      "正在下载37篇\n",
      "正在下载38篇\n",
      "正在下载39篇\n",
      "正在下载40篇\n",
      "正在下载41篇\n",
      "正在下载42篇\n",
      "正在下载43篇\n",
      "正在下载44篇\n",
      "正在下载45篇\n",
      "正在下载46篇\n",
      "正在下载47篇\n",
      "正在下载48篇\n",
      "正在下载49篇\n",
      "正在下载50篇\n",
      "正在下载51篇\n",
      "正在下载52篇\n",
      "正在下载53篇\n",
      "正在下载54篇\n",
      "正在下载55篇\n",
      "正在下载56篇\n",
      "正在下载57篇\n",
      "正在下载58篇\n",
      "正在下载59篇\n",
      "正在下载60篇\n",
      "正在下载61篇\n",
      "正在下载62篇\n",
      "正在下载63篇\n",
      "正在下载64篇\n",
      "正在下载65篇\n",
      "正在下载66篇\n",
      "正在下载67篇\n",
      "正在下载68篇\n",
      "正在下载69篇\n"
     ]
    }
   ],
   "source": [
    "#reda url-content\n",
    "import xlrd\n",
    "import urllib\n",
    "import re\n",
    "import jieba\n",
    "import time \n",
    "\n",
    "def SaveNews(path,str):\n",
    "    #path指的是文件存储的文件位置及文件名\n",
    "    #str是需要存储的内容，追加的方式存储\n",
    "    txtfile = open(path,'a+')\n",
    "    print(str,file=txtfile)\n",
    "    txtfile.close()\n",
    "\n",
    "def clearData(Data):\n",
    "    pattern = re.compile(r'[^\\u4e00-\\u9fa5]')\n",
    "    Data = re.sub(pattern,\"\",Data)\n",
    "    Data = re.sub(r'新华社',\"\",Data)\n",
    "    return Data\n",
    "def collectContent(NewPath):\n",
    "    header = {\n",
    "        \"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.141 Safari/537.36 Edg/87.0.664.75\"\n",
    "    }\n",
    "    workbook = xlrd.open_workbook(NewPath)\n",
    "    urlsheet = workbook.sheet_by_index(0)\n",
    "    print(\"共%d篇新闻稿\"%(urlsheet.nrows-1))\n",
    "    for rx in range(1,urlsheet.nrows):\n",
    "        newsUrl = urlsheet.row(rx)[0].value\n",
    "        req = urllib.request.Request(url=newsUrl,headers=header)\n",
    "        res = urllib.request.urlopen(req)\n",
    "        html = res.read().decode(\"utf-8\")\n",
    "        pageContent = BeautifulSoup(html,\"html.parser\")\n",
    "        pageContent = str(pageContent)\n",
    "        finalContent = clearData(pageContent)\n",
    "        time.sleep(1)\n",
    "        print(\"正在下载%d篇\"%(rx))\n",
    "        SaveNews(\"AnHuiNews.txt\",finalContent)\n",
    "    print(\"下载完成,可进行下一步分析\")\n",
    "collectContent(\"安徽新闻网链接.xls\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}